stages:
  sync_negative_train_samples:
    cmd: aws s3 cp s3://ezra-ml-dvc/reporter/df_negative_for_train_and_stats.csv ./data/df_negative_for_train_and_stats.csv
    deps:
    - s3://ezra-ml-dvc/reporter/df_negative_for_train_and_stats.csv
    outs:
    - data/df_negative_for_train_and_stats.csv
  sync_positive_train_samples:
    cmd: aws s3 cp s3://ezra-ml-dvc/reporter/df_positive_w_id_for_train_and_stats.csv
      ./data/df_positive_w_id_for_train_and_stats.csv
    deps:
    - s3://ezra-ml-dvc/reporter/df_positive_w_id_for_train_and_stats.csv
    outs:
    - data/df_positive_w_id_for_train_and_stats.csv
  sync_snippet_database:
    cmd: aws s3 cp s3://ezra-ml-dvc/reporter/cleaned_snippets_with_org_name_new_rows.csv
      ./data/cleaned_snippets_with_org_name_new_rows.csv
    deps:
    - s3://ezra-ml-dvc/reporter/cleaned_snippets_with_org_name_new_rows.csv
    outs:
    - data/cleaned_snippets_with_org_name_new_rows.csv
  sync_uml_sbert:
    cmd: aws s3 cp s3://ezra-ml-dvc/reporter/UMLSBert ./data/UMLS --recursive
    deps:
    - s3://ezra-ml-dvc/reporter/UMLSBert/config.json
    - s3://ezra-ml-dvc/reporter/UMLSBert/pytorch_model.bin
    - s3://ezra-ml-dvc/reporter/UMLSBert/vocab.txt
    outs:
    - data/UMLS/config.json
    - data/UMLS/pytorch_model.bin
    - data/UMLS/vocab.txt
  sync_bio_sent:
    cmd: aws s3 cp s3://ezra-ml-dvc/reporter/bioSent2Vec.bin ./data/bioSent2Vec.bin
    deps:
    - s3://ezra-ml-dvc/reporter/bioSent2Vec.bin
    outs:
    - data/bioSent2Vec.bin
  embed_biosent_snippets:
    cmd: docker-compose run ml.thageesan "python -m ai.data_pipeline.embed_snippets_biosent
      ."
    deps:
    - ai/data_pipeline/embed_snippets_biosent/__init__.py
    - ai/data_pipeline/embed_snippets_biosent/__main__.py
    - data/bioSent2Vec.bin
    - data/training_samples.parquet
    outs:
    - data/embed_snippets_biosent.parquet
  embed_umlsbert_snippets:
    cmd: docker-compose run ml.thageesan "python -m ai.data_pipeline.embed_snippets_umlsbert
      ."
    deps:
    - ai/data_pipeline/embed_snippets_umlsbert/__init__.py
    - ai/data_pipeline/embed_snippets_umlsbert/__main__.py
    - data/UMLS/config.json
    - data/UMLS/pytorch_model.bin
    - data/UMLS/vocab.txt
    - data/training_samples.parquet
    outs:
    - data/embed_snippets_umls.parquet
  generate_training_samples:
    cmd: docker-compose run ml.thageesan "python -m ai.data_pipeline.generate_training_samples
      ."
    deps:
    - ai/data_pipeline/generate_training_samples/__init__.py
    - ai/data_pipeline/generate_training_samples/__main__.py
    - data/df_negative_for_train_and_stats.csv
    - data/df_positive_w_id_for_train_and_stats.csv
    outs:
    - data/training_samples.parquet
  generate_corpus:
    cmd: docker-compose run ml.thageesan "python -m ai.data_pipeline.generate_corpus
      ."
    deps:
    - ai/data_pipeline/generate_corpus/__init__.py
    - ai/data_pipeline/generate_corpus/__main__.py
    - data/cleaned_snippets_with_org_name_new_rows.csv
    outs:
    - data/corpus.npy
  embed_umlsbert_finding:
    cmd: docker-compose run ml.thageesan "python -m ai.data_pipeline.embed_finding_umlsbert
      ."
    deps:
    - ai/data_pipeline/embed_finding_umlsbert/__init__.py
    - ai/data_pipeline/embed_finding_umlsbert/__main__.py
    - data/training_samples.parquet
    outs:
    - data/embed_finding_umls.parquet
  embed_biosent_finding:
    cmd: docker-compose run ml.thageesan "python -m ai.data_pipeline.embed_finding_biosent
      ."
    deps:
    - ai/data_pipeline/embed_finding_biosent/__init__.py
    - ai/data_pipeline/embed_finding_biosent/__main__.py
    - data/training_samples.parquet
    outs:
    - data/embed_finding_biosent.parquet
  extract_features:
    cmd: docker-compose run ml.thageesan "python -m ai.data_pipeline.feature_extraction
      ."
    deps:
    - ai/data_pipeline/feature_extraction/__init__.py
    - ai/data_pipeline/feature_extraction/__main__.py
    - data/corpus.npy
    - data/embed_finding_biosent.parquet
    - data/embed_finding_umls.parquet
    - data/embed_numbers_umls.parquet
    - data/embed_snippets_biosent.parquet
    - data/embed_snippets_umls.parquet
    - data/training_samples.parquet
    outs:
    - data/feature_extraction.parquet
  embed_number_in_finding:
    cmd: docker-compose run ml.thageesan "python -m ai.data_pipeline.embed_numbers_umls
      ."
    deps:
    - ai/data_pipeline/embed_numbers_umls/__init__.py
    - ai/data_pipeline/embed_numbers_umls/__main__.py
    - data/training_samples.parquet
    outs:
    - data/embed_numbers_umls.parquet
  train_test_split:
    cmd: docker-compose run ml.thageesan "python -m ai.data_pipeline.train_test_split
      ."
    deps:
    - ai/data_pipeline/train_test_split/__init__.py
    - ai/data_pipeline/train_test_split/__main__.py
    - data/feature_extraction_training.parquet
    params:
    - params.py:
      - TestTrainSetConfig
    outs:
    - data/testing_set.parquet
    - data/training_set.parquet
